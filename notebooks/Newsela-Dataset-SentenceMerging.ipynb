{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be1808f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from alignment_utils import get_components, count_alignment_types\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from os import path\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27bdb3c",
   "metadata": {},
   "source": [
    "# Newsela Sentence Merging\n",
    "The Newsela corpus provided does not reflect the sentence alignments properly, where e.g., in a 1-n alignment the training instance contains 1 complex sentence -> 2 simple sentences. Also for aligned data, no train/test/dev splits are done properly.\n",
    "This notebook does following:\n",
    "1. Loads the aligned sentence pairs\n",
    "2. Loads the unaligned train/test/dev splits\n",
    "3. Assigns the original splits to the aligned sentence pairs\n",
    "4. Filters some data and removes some readability levels like the authors of Newsela did\n",
    "5. Merges aligned sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7327e2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "newsela_base_dir = \"/homes/julez/datasets-raw/newsela-auto/newsela-auto/\"\n",
    "output_dir = \"datasets-raw/newsela_test/\"\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1dcf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17894f79",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "This loads the aligned data from the Newsela corpus. Some lines contain multiple tabstops, therefore this is read and split manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329e41bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path.join(newsela_base_dir, \"all_data/aligned-sentence-pairs-all.tsv\")) as file:\n",
    "    data = []\n",
    "    for row in file.readlines():\n",
    "        data.append(row.strip().split('\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7ddd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['simple_sent_id', 'simple_sent', 'complex_sent_id', 'complex_sent']\n",
    "df_aligned = pd.DataFrame(data, columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a333e55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aligned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8313a07d",
   "metadata": {},
   "source": [
    "# Reproduce splits\n",
    "Reproduce Train/Test/Dev splits from ACL2020 Paper. The original dataset doesn't provide sentence IDs or Document IDs for the train/test/dev splits, only the sentences. Therefore we search for each sent in the splits to regain the split information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907f88f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_split_data(split: str):\n",
    "    with open(path.join(newsela_base_dir, f\"ACL2020/{split}.src\")) as file:\n",
    "        src = []\n",
    "        for row in file.read().splitlines():\n",
    "            src.append(row.strip())\n",
    "        \n",
    "    with open(path.join(newsela_base_dir, f\"ACL2020/{split}.dst\")) as file:\n",
    "        tgt = []\n",
    "        for row in file.read().splitlines():\n",
    "            tgt.append(row.strip())\n",
    "\n",
    "    return pd.DataFrame({\"complex_sent\": src, \"simple_sent\": tgt, \"split\": split})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc949aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data and create one DF with complex - simple sents and split\n",
    "df_train = read_split_data(\"train\")\n",
    "df_test = read_split_data(\"test\")\n",
    "df_valid = read_split_data(\"valid\")\n",
    "\n",
    "df_splits = pd.concat([df_train, df_test, df_valid], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f30c022",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a0501b",
   "metadata": {},
   "source": [
    "Since the text in ACL2020 DS is processed and punctuation is surrounded with whitespace we cannot match directly.\n",
    "Solution: Complex and Simple is tokenized to remove punctuation and then hashed.\n",
    "This is done for the splits and the auto aligned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d568d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df_splits[\"complex_tokenized\"] = df_splits.progress_apply(\n",
    "    lambda row: tokenizer.tokenize(row[\"complex_sent\"]), axis=1)\n",
    "df_splits[\"simple_tokenized\"] = df_splits.progress_apply(\n",
    "    lambda row: tokenizer.tokenize(row[\"simple_sent\"]), axis=1)\n",
    "df_splits[\"hash\"] = df_splits.progress_apply(\n",
    "    lambda row: hash(\" \".join(row[\"complex_tokenized\"] + row[\"simple_tokenized\"])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1335ff79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aligned[\"complex_tokenized\"] = df_aligned.progress_apply(\n",
    "    lambda row: tokenizer.tokenize(row[\"complex_sent\"]), axis=1)\n",
    "df_aligned[\"simple_tokenized\"] = df_aligned.progress_apply(\n",
    "    lambda row: tokenizer.tokenize(row[\"simple_sent\"]), axis=1)\n",
    "df_aligned[\"hash\"] = df_aligned.progress_apply(\n",
    "    lambda row: hash(\" \".join(row[\"complex_tokenized\"] + row[\"simple_tokenized\"])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531bfb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6603110e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then merge and remove temp columns\n",
    "# Rename merged columns\n",
    "df_auto_all = df_aligned.merge(df_splits, on=\"hash\")\n",
    "df_auto_all = df_auto_all.drop(columns=[\n",
    "    \"complex_tokenized_x\",\n",
    "    \"simple_tokenized_x\",\n",
    "    \"complex_sent_y\",\n",
    "    \"simple_sent_y\",\n",
    "    \"complex_tokenized_y\",\n",
    "    \"simple_tokenized_y\"\n",
    "])\n",
    "df_auto_all = df_auto_all.rename(columns={\n",
    "    \"simple_sent_x\": \"simple_sent\",\n",
    "    \"complex_sent_x\": \"complex_sent\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f15e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_auto_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bee1039",
   "metadata": {},
   "source": [
    "# Filter Data\n",
    "The original paper filters out:\n",
    "* Instances where Simple == Complex\n",
    "* Removed some readability level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db3849a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set article ID\n",
    "df_auto_all['article_id'] = df_auto_all.apply(lambda x: re.search('^\\w*-?\\w*\\.?\\w*', x['simple_sent_id']).group(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95a7947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 666k mentioned in paper\n",
    "df_filtered = df_auto_all[df_auto_all['simple_sent'] != df_auto_all['complex_sent']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032dec88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a541f8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Reading Level Transition\n",
    "simple_rl = df_filtered['simple_sent_id'].str.split(pat='-', expand=True)\n",
    "complex_rl = df_filtered['complex_sent_id'].str.split(pat='-', expand=True)\n",
    "\n",
    "#df_uni['rl_test'] = pd.concat([complex_rl[2], simple_rl[2]], axis = 1).apply(lambda x: '-'.join(x))\n",
    "\n",
    "df_filtered['simple_rl'] = simple_rl[2]\n",
    "df_filtered['complex_rl'] = complex_rl[2]\n",
    "df_filtered['rl_transition'] = df_filtered[['complex_rl', 'simple_rl']].apply(lambda row: '-'.join(row.values.astype(str)), axis=1)\n",
    "df_filtered = df_filtered.drop(['simple_rl', 'complex_rl'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08854a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Readability Levels 0-1, 1-2, 2-3\n",
    "# Keep 0-2, 0-3, 1-3, 2-3, 0-4, 1-4, 2-4, 3-4\n",
    "# 481k mentioned in paper -> 482k\n",
    "filters = ['0-1', '1-2', '2-3']\n",
    "transitions = [\"0-2\", \"0-3\", \"0-4\", \"1-3\", \"1-4\", \"2-4\", \"3-4\"]\n",
    "df_filtered = df_filtered[~df_filtered.rl_transition.isin(filters)]\n",
    "df_filtered[\"is_aligned\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192ead25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to get Text Simplfication Phenomenon Stats\n",
    "transitions = [\"0-1\", \"0-2\", \"0-3\", \"0-4\", \"1-2\", \"1-3\", \"1-4\", \"2-3\", \"2-4\", \"3-4\"]\n",
    "df_filtered[\"is_aligned\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96783b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {}\n",
    "for transition in transitions:\n",
    "    dataframes[f\"df_filtered_{transition}\"] = df_filtered[df_filtered[\"rl_transition\"] == transition]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b818d1",
   "metadata": {},
   "source": [
    "# Get Alignments for Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48eebda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: <doc_id.lang>-<level>-<par_id>-<sent_id>\n",
    "# level 1 == complex, level 0 == simple\n",
    "#DOC_ID = lambda x: x.split('-')[-5:-3]\n",
    "DOC_ID = lambda x: re.search('^\\w*-?\\w*\\.?\\w*', x).group()\n",
    "LEVEL = lambda x: x.split('-')[-3]\n",
    "SENT_ID = lambda x: int(x.split('-')[-1]) # cast to int for numeric ordering\n",
    "IS_COMPLEX = lambda x, y: LEVEL(x) == y\n",
    "GET_COMPLEXITY_LVL = lambda x: re.search(r'\\d+', x).group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ee7caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corresponding_nodes(components: set, complexity_level) -> list:\n",
    "    output = []\n",
    "    for component in components:\n",
    "        alignment = {\"complex\": [], \"simple\": []}\n",
    "\n",
    "        for key in component:\n",
    "            if IS_COMPLEX(key, complexity_level):\n",
    "                alignment[\"complex\"].append(key)\n",
    "            else:\n",
    "                alignment[\"simple\"].append(key)\n",
    "        \n",
    "        # Assume that Sentence IDs have asc order\n",
    "        alignment[\"complex\"].sort()\n",
    "        alignment[\"simple\"].sort()\n",
    "\n",
    "        output.append(alignment)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a46bfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sents(nodes: list[dict], df):\n",
    "    d = {\n",
    "        \"complex_sent_ids\": [],\n",
    "        \"complex_sent\": [],\n",
    "        \"simple_sent_ids\": [],\n",
    "        \"simple_sent\": [],\n",
    "        \"split\": [],\n",
    "    }\n",
    "    \n",
    "    for node in tqdm(nodes):\n",
    "        # Extract + concat complex sents\n",
    "        d[\"complex_sent_ids\"].append(node[\"complex\"])\n",
    "        complex_sents = []\n",
    "        for sent_id in node[\"complex\"]: \n",
    "            row = df[df[\"complex_sent_id\"] == sent_id]\n",
    "            complex_sents.append(row[\"complex_sent\"].iloc[0])\n",
    "        d[\"complex_sent\"].append(\" \".join(complex_sents))\n",
    "\n",
    "        # Extract + concat simple sents\n",
    "        d[\"simple_sent_ids\"].append(node[\"simple\"])\n",
    "        simple_sents = []\n",
    "        for sent_id in node[\"simple\"]: \n",
    "            row = df[df[\"simple_sent_id\"] == sent_id]\n",
    "            simple_sents.append(row[\"simple_sent\"].iloc[0])\n",
    "            split = row[\"split\"].iloc[0]\n",
    "\n",
    "        d[\"simple_sent\"].append(\" \".join(simple_sents))\n",
    "        d[\"split\"].append(split)\n",
    "        \n",
    "    df_output = pd.DataFrame(data=d)\n",
    "    \n",
    "    return df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3478b94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_df = dataframes[\"df_filtered_0-2\"]\n",
    "orig_df = orig_df[:1000]\n",
    "df_key = \"df_filtered_0-2\"\n",
    "\n",
    "df_out = pd.DataFrame()\n",
    "alignments = list(orig_df[['complex_sent_id', 'simple_sent_id', 'is_aligned']].itertuples(index=False))\n",
    "components = get_components(alignments)\n",
    "nodes = get_corresponding_nodes(components, GET_COMPLEXITY_LVL(df_key))\n",
    "df_out = pd.concat([df_out, merge_sents(nodes, orig_df)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd5d0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This iterates over DataFrames for all Transitions (Readability Level 0-2, ...)\n",
    "# Computes the alignments and gets corresponding sentences\n",
    "# All aligned sentences are then added to one DataFrame\n",
    "df_out = pd.DataFrame()\n",
    "for df_key, orig_df in dataframes.items():\n",
    "    print(\"---\")\n",
    "    print(df_key)\n",
    "    alignments = list(orig_df[['complex_sent_id', 'simple_sent_id', 'is_aligned']].itertuples(index=False))\n",
    "    components = get_components(alignments)\n",
    "    nodes = get_corresponding_nodes(components, GET_COMPLEXITY_LVL(df_key))\n",
    "    df_out = pd.concat([df_out, merge_sents(nodes, orig_df)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51352a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write all data\n",
    "df_out.to_csv(\"datasets-raw/newsela_test/aligned-sentence-pairs-merged.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "660382c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/homes/julez/ts-adapters/src/data/newsela/aligned-sentence-pairs-merged.csv\", index_col=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdcc5877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>complex_sent_ids</th>\n",
       "      <th>complex_sent</th>\n",
       "      <th>simple_sent_ids</th>\n",
       "      <th>simple_sent</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['brain-gender.en-0-4-0']</td>\n",
       "      <td>To figure this out, the team — led by psychobi...</td>\n",
       "      <td>['brain-gender.en-2-8-0']</td>\n",
       "      <td>To figure out more, the team did more research...</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['brain-gender.en-0-4-1']</td>\n",
       "      <td>In other words, they looked for examples of me...</td>\n",
       "      <td>['brain-gender.en-2-8-1']</td>\n",
       "      <td>They looked for measurements that appeared to ...</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['brain-gender.en-0-4-2']</td>\n",
       "      <td>Then, after identifying these elements, the re...</td>\n",
       "      <td>['brain-gender.en-2-8-2']</td>\n",
       "      <td>After identifying the elements, researchers lo...</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['brain-gender.en-0-5-1']</td>\n",
       "      <td>On these scans, they examined 116 separate reg...</td>\n",
       "      <td>['brain-gender.en-2-11-1', 'brain-gender.en-2-...</td>\n",
       "      <td>On their scans, researchers examined 116 separ...</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['brain-gender.en-0-5-2']</td>\n",
       "      <td>In each case, the 281 scans were divided into ...</td>\n",
       "      <td>['brain-gender.en-2-11-4']</td>\n",
       "      <td>In each case, the scans were divided into thre...</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299226</th>\n",
       "      <td>['bbking-obit.en-3-21-3']</td>\n",
       "      <td>When he found out that the men had been fighti...</td>\n",
       "      <td>['bbking-obit.en-4-11-6', 'bbking-obit.en-4-11...</td>\n",
       "      <td>When he found out that the men had been fighti...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299227</th>\n",
       "      <td>['bbking-obit.en-3-22-0']</td>\n",
       "      <td>King's guitar was stolen two years later, but ...</td>\n",
       "      <td>['bbking-obit.en-4-12-0']</td>\n",
       "      <td>From then on, King named every guitar he owned...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299228</th>\n",
       "      <td>['bbking-obit.en-3-23-0']</td>\n",
       "      <td>B.B. King recorded more than 50 albums through...</td>\n",
       "      <td>['bbking-obit.en-4-21-0']</td>\n",
       "      <td>King recorded more than 50 albums.</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299229</th>\n",
       "      <td>['bbking-obit.en-3-23-1']</td>\n",
       "      <td>He once said he had lost count of how many rec...</td>\n",
       "      <td>['bbking-obit.en-4-21-1']</td>\n",
       "      <td>At one time, he said he could not remember how...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299230</th>\n",
       "      <td>['bbking-obit.en-3-23-3']</td>\n",
       "      <td>He received many other musical awards.</td>\n",
       "      <td>['bbking-obit.en-4-21-3']</td>\n",
       "      <td>He received many other music awards as well fo...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>299231 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 complex_sent_ids   \n",
       "0       ['brain-gender.en-0-4-0']  \\\n",
       "1       ['brain-gender.en-0-4-1']   \n",
       "2       ['brain-gender.en-0-4-2']   \n",
       "3       ['brain-gender.en-0-5-1']   \n",
       "4       ['brain-gender.en-0-5-2']   \n",
       "...                           ...   \n",
       "299226  ['bbking-obit.en-3-21-3']   \n",
       "299227  ['bbking-obit.en-3-22-0']   \n",
       "299228  ['bbking-obit.en-3-23-0']   \n",
       "299229  ['bbking-obit.en-3-23-1']   \n",
       "299230  ['bbking-obit.en-3-23-3']   \n",
       "\n",
       "                                             complex_sent   \n",
       "0       To figure this out, the team — led by psychobi...  \\\n",
       "1       In other words, they looked for examples of me...   \n",
       "2       Then, after identifying these elements, the re...   \n",
       "3       On these scans, they examined 116 separate reg...   \n",
       "4       In each case, the 281 scans were divided into ...   \n",
       "...                                                   ...   \n",
       "299226  When he found out that the men had been fighti...   \n",
       "299227  King's guitar was stolen two years later, but ...   \n",
       "299228  B.B. King recorded more than 50 albums through...   \n",
       "299229  He once said he had lost count of how many rec...   \n",
       "299230             He received many other musical awards.   \n",
       "\n",
       "                                          simple_sent_ids   \n",
       "0                               ['brain-gender.en-2-8-0']  \\\n",
       "1                               ['brain-gender.en-2-8-1']   \n",
       "2                               ['brain-gender.en-2-8-2']   \n",
       "3       ['brain-gender.en-2-11-1', 'brain-gender.en-2-...   \n",
       "4                              ['brain-gender.en-2-11-4']   \n",
       "...                                                   ...   \n",
       "299226  ['bbking-obit.en-4-11-6', 'bbking-obit.en-4-11...   \n",
       "299227                          ['bbking-obit.en-4-12-0']   \n",
       "299228                          ['bbking-obit.en-4-21-0']   \n",
       "299229                          ['bbking-obit.en-4-21-1']   \n",
       "299230                          ['bbking-obit.en-4-21-3']   \n",
       "\n",
       "                                              simple_sent  split  \n",
       "0       To figure out more, the team did more research...  valid  \n",
       "1       They looked for measurements that appeared to ...  valid  \n",
       "2       After identifying the elements, researchers lo...  valid  \n",
       "3       On their scans, researchers examined 116 separ...  valid  \n",
       "4       In each case, the scans were divided into thre...  valid  \n",
       "...                                                   ...    ...  \n",
       "299226  When he found out that the men had been fighti...  train  \n",
       "299227  From then on, King named every guitar he owned...  train  \n",
       "299228                 King recorded more than 50 albums.  train  \n",
       "299229  At one time, he said he could not remember how...  train  \n",
       "299230  He received many other music awards as well fo...  train  \n",
       "\n",
       "[299231 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a364d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[df[\"split\"] == \"train\"]\n",
    "test = df[df[\"split\"] == \"test\"]\n",
    "validation = df[df[\"split\"] == \"valid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a2441e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"/homes/julez/ts-adapters/src/data/newsela/train.csv\")\n",
    "test.to_csv(\"/homes/julez/ts-adapters/src/data/newsela/test.csv\")\n",
    "validation.to_csv(\"/homes/julez/ts-adapters/src/data/newsela/valid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feccc097",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
